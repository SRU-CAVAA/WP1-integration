[Experiment]
#webots_worlds = 0:Open Arena	1:Linear track		2:Tmaze 		3:Double Tmaze
webots_world = 2
total_trials = 1000
exp_batch_size = 20
trial_time_limit = 0
frame_limit = 40000
plotting = 0
rand_start = False


[Agent]
#agent_mode: 1-> Reactive 	2-> Adaptive 	3-> Contextual
agent_mode: 3


[Reactive_Layer]
#activated = 1
#modes: 0 -> Continuous 	1-> 4 actions 	2-> 8 actions 	3-> 16 actions 	4-> 9 actions (forward bias)
reactive_layer_mode = 6
obstacle_avoidance = True
avoidance_dist = 0.05
state_action_rate = 1

homeostasis = True
homeo_discount = 0.0001

img_saving = False

action_space_mode_0 = [6.28, 6.28]
action_space_mode_1 = [[6.28, 6.28], [-6.28, -6.28], [-6.28, 6.28], [6.28, -6.28]]
action_space_mode_2 = [[6.28, 6.28], [-6.28, -6.28], [-6.28, 6.28], [6.28, -6.28], [3.14, 6.28], [6.28, 3.14], [-3.14, -6.28], [-6.28, -3.14]]
action_space_mode_3 = [[6.28, 6.28], [-6.28, -6.28], [-6.28, 6.28], [6.28, -6.28], [3.14, 6.28], [6.28, 3.14], [-3.14, -6.28], [-6.28, -3.14], [1.57, 6.28], [6.28, 1.57], [-1.57, -6.28], [-6.28, -1.57], [4.71, 6.28], [6.28, 4.71], [-4.71, -6.28], [-6.28, -4.71]]
action_space_mode_4 = [[-6.28, 6.28], [1.57, 6.28], [3.14, 6.28], [4.71, 6.28], [6.28, 6.28], [6.28, 4.71], [6.28, 3.14],  [6.28, 1.57], [6.28, -6.28]]
action_space_mode_5 = [[-1.57, 1.57], [1.57, 3.14], [3.14, 4.71], [4.71, 6.28], [6.28, 6.28], [6.28, 4.71], [4.71, 3.14],  [3.14, 1.57], [1.57, -1.57]]
action_space_mode_6 = [[-0.79, 0.79], [0.0, 1.57], [1.57, 3.14], [3.14, 4.71], [3.93, 4.71], [4.71, 6.28], [5.5, 6.28], [6.28, 6.28], [6.28, 5.5], [6.28, 4.71], [4.71, 3.93], [4.71, 3.14],  [3.14, 1.57], [1.57, 0.0], [0.79, -0.79]]


[Adaptive_Layer]
overrepresented_autoencoder = False
motivational_autoencoder = True
AE_retrain = False

motivational_input_lenght = 1000
batch_size = 16
n_hidden = [200, 100, 200, 400]
C_factor = 1000
learning_rate = 1e-6
alpha = 1e5
# OpenArena_200nHidden_2000Epochs_1e-05LR or OpenArena_200nHidden_2000Epochs_1e-05LR4_bias
ae_models = [OpenArena_200nHidden_2000Epochs_1e-05LR.pth, OpenArena_200nHidden_2000Epochs_1e-05LR4.0OR_bias.pth, Tmaze_Allo_200nHidden_1000motivI_1000Epochs_1e-05LR.pth, Tmaze_200nHidden_500Epochs_1e-05LR.pth, DoubleTmaze_Allo_400nHidden_1000motivI_1000Epochs_1e-05LR.pth, DoubleTmaze_400nHidden_500Epochs_1e-05LR.pth]


[Contextual_Layer]
load_LTM = False
STM_limit = 50
LTM_limit = 200

alpha_tr = 0.005
coll_thres_abs = 0.95
coll_thres_prop = 0.95
tau_decay = 0.9



[Environment]
reset_reward_margin = 0.1

openarena_epuck_translation = -1.15 1.15 0.0
openarena_epuck_rotation = 0.0 0.0 1.0 -0.785
openarena_green_reward_X = 0
openarena_green_reward_Y = 0

lineartrack_epuck_translation = 0.0 -1.2 0.0
lineartrack_epuck_rotation = 0.0 0.0 1.0 1.57
lineartrack_green_reward_X = 0
lineartrack_green_reward_Y = 0.7

tmaze_epuck_translation = 0.0 -0.5 0.3
tmaze_epuck_rotation = 0.0 0.0 1.0 1.57
tmaze_red_reward_X = -0.55
tmaze_red_reward_Y = 0.45
tmaze_blue_reward_X = 0.55
tmaze_blue_reward_Y = 0.45

double_tmaze_epuck_translation = 0 0 0.0
double_tmaze_epuck_rotation = 0.0 0.0 1.0 1.57
double_tmaze_red_reward_X = -1.15
double_tmaze_red_reward_Y = 1.0
double_tmaze_blue_reward_X = 1.15
double_tmaze_blue_reward_Y = 1.0
double_tmaze_purple_reward_X = -1.15
double_tmaze_purple_reward_Y = -1.0
double_tmaze_orange_reward_X = 1.15
double_tmaze_orange_reward_Y = -1.0

[Data_gathering]
data_classes = [Trial_number, Trial_time, Frame_ID, Embedding, action, retrieved_action, X, Y, Z, Internal_states, RewardID, Reward_value, n_replays]
save_ae_model = True